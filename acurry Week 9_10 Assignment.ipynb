{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   File: DSC550 Curry Week9/10 MidTerm.py\n",
    "#   Name: Adam Curry\n",
    "#   Date: 05/13/2020\n",
    "#   Course: DSC540 - Data Mining\n",
    "#   Desc: This program is my first neural network \n",
    "#   Usage: This program should be used when reviewing week 9/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# import the covid comments\n",
    "path = r\"C:\\Users\\adamp\\OneDrive\\Desktop\\a_data_mining\\Week9\\categorized-comments.jsonl\"\n",
    "df = pd.read_json(path, lines=True,  encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply text cleanup and normailization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply stemming, remove puncuation, lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    remove punct and apply stemming and lower case all text\n",
    "    \"\"\"\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '').lower()\n",
    "        stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df['txt_no_punk'] = df['txt'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cat                                                txt  \\\n",
      "0  sports  Barely better than Gabbert? He was significant...   \n",
      "1  sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4  sports                                      No!! NOO!!!!!   \n",
      "\n",
      "                                         txt_no_punk  \\\n",
      "0  barely better than gabbert he was significantl...   \n",
      "1  fuck the ducks and the angels but welcome to a...   \n",
      "2  should have drafted more wrs\\n\\n matt millen p...   \n",
      "3                       donehttpsiimgurcom2yz90pmjpg   \n",
      "4                                             no noo   \n",
      "\n",
      "                                          Lemmatized  \n",
      "0  barely good than gabbert he be significantly w...  \n",
      "1  fuck the duck and the angel but welcome to all...  \n",
      "2    should have draft more wrs matt millen probably  \n",
      "3                       donehttpsiimgurcom2yz90pmjpg  \n",
      "4                                             no noo  \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# add the models to an object\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    \"\"\"\n",
    "    apply part of speech tagging\n",
    "    \"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    apply part of speech tagging\n",
    "    \"\"\"\n",
    "    global wordnet_tagged\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "# Lemmatizing\n",
    "df['Lemmatized'] = df['txt_no_punk'].apply(lambda x: lemmatize_sentence(x))\n",
    "#df = df[df['Lemmatized'].apply(lambda x: len(x) > 10)]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply vectorization TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 238)\t0.23067614456683855\n",
      "  (0, 3722)\t0.18828809309456476\n",
      "  (0, 384)\t0.18254620603252011\n",
      "  (0, 867)\t0.1558602661088407\n",
      "  (0, 4838)\t0.20209144726532718\n",
      "  (0, 1546)\t0.14621609845316993\n",
      "  (0, 3737)\t0.22108733722010077\n",
      "  (0, 3335)\t0.19923774811481318\n",
      "  (0, 2375)\t0.2275824328298333\n",
      "  (0, 2561)\t0.15686305162093123\n",
      "  (0, 2408)\t0.07801989787589841\n",
      "  (0, 803)\t0.19212057462730373\n",
      "  (0, 1375)\t0.16151263064204519\n",
      "  (0, 2503)\t0.1570278428419804\n",
      "  (0, 813)\t0.1918818643682858\n",
      "  (0, 3028)\t0.1954816178393946\n",
      "  (0, 4341)\t0.21337169172815756\n",
      "  (0, 515)\t0.11606863336046308\n",
      "  (0, 4391)\t0.21927493939510231\n",
      "  (0, 3740)\t0.22355167978193927\n",
      "  (0, 2546)\t0.08113318867721633\n",
      "  (0, 1998)\t0.12524034164209039\n",
      "  (0, 300)\t0.18273186363270963\n",
      "  (0, 3538)\t0.18134205056805425\n",
      "  (0, 3155)\t0.23223089618308873\n",
      "  :\t:\n",
      "  (606472, 1847)\t0.10044998295732523\n",
      "  (606472, 960)\t0.14200574304339786\n",
      "  (606472, 3562)\t0.12678625065868954\n",
      "  (606472, 4455)\t0.1368752365056512\n",
      "  (606472, 4456)\t0.2305557926120909\n",
      "  (606472, 2408)\t0.09912238423562626\n",
      "  (606473, 1793)\t0.4410018134254372\n",
      "  (606473, 143)\t0.42160747288553463\n",
      "  (606473, 17)\t0.38376459854642386\n",
      "  (606473, 3818)\t0.5220241334422737\n",
      "  (606473, 2082)\t0.3627111339066548\n",
      "  (606473, 2408)\t0.19161423345462297\n",
      "  (606473, 2546)\t0.19926037048706\n",
      "  (606474, 3647)\t1.0\n",
      "  (606475, 4509)\t0.41026678361486413\n",
      "  (606475, 124)\t0.3087240648224351\n",
      "  (606475, 3601)\t0.31603001523397223\n",
      "  (606475, 3437)\t0.2878843435926917\n",
      "  (606475, 3735)\t0.2968504145491969\n",
      "  (606475, 3029)\t0.2882113645664889\n",
      "  (606475, 4901)\t0.2698969285189352\n",
      "  (606475, 3077)\t0.2568102650986126\n",
      "  (606475, 3513)\t0.38693090180257017\n",
      "  (606475, 1847)\t0.27266933878870336\n",
      "  (606475, 2546)\t0.13990117081263462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english',max_features=5000) \n",
    "dtm = tfidf.fit_transform(df['Lemmatized'])\n",
    "df['tfidf'] = tfidf.fit_transform(df['Lemmatized'])\n",
    "\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424533, 5000)\n",
      "(181943, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dtm\n",
    "y = df.cat\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Network Classifier with Scikit - Fit multi classifier and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# set the number of layers to match the count of features\n",
    "# specify ReLU as the activation method\n",
    "# adam - refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,10,20),\n",
    "                    activation='relu', \n",
    "                    solver='adam', max_iter=500)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show the accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10525    249   6905]\n",
      " [    86  63347  38506]\n",
      " [  1294   7687 295934]]\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.88      0.60      0.71     17679\n",
      "                sports       0.89      0.62      0.73    101939\n",
      "           video_games       0.87      0.97      0.92    304915\n",
      "\n",
      "             micro avg       0.87      0.87      0.87    424533\n",
      "             macro avg       0.88      0.73      0.79    424533\n",
      "          weighted avg       0.87      0.87      0.86    424533\n",
      "\n",
      "[[  3735    238   3459]\n",
      " [   138  24196  19550]\n",
      " [  1111   6137 123379]]\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.75      0.50      0.60      7432\n",
      "                sports       0.79      0.55      0.65     43884\n",
      "           video_games       0.84      0.94      0.89    130627\n",
      "\n",
      "             micro avg       0.83      0.83      0.83    181943\n",
      "             macro avg       0.79      0.67      0.71    181943\n",
      "          weighted avg       0.83      0.83      0.82    181943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# print the confusion matrix and the trained and tested model's performance\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(classification_report(y_train,predict_train))\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(classification_report(y_test,predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Neural Network Classifier with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert categorical data to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "y = lb_make.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424533, 5000)\n",
      "(181943, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu', input_dim=5000))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "424533/424533 [==============================] - 419s 987us/step - loss: 0.3764 - accuracy: 0.8453\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "kmodel = model.fit(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.8602888584136963% \n",
      " Error on training data: 0.1397111415863037\n",
      "Accuracy on test data: 0.8437477946281433% \n",
      " Error on test data: 0.1562522053718567\n"
     ]
    }
   ],
   "source": [
    "pred_train= model.predict(X_train)\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1]))   \n",
    " \n",
    "pred_test= model.predict(X_test)\n",
    "scores2 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2[1], 1 - scores2[1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Classifying Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, re, pickle\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color channel value will be first\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for consistency\n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set image params\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and targer fromm MNIST data\n",
    "(data_train, target_train),(data_test,target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the pixel intensity to be between 0-1\n",
    "features_train = data_train/255\n",
    "features_test = data_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one hot encoding to target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# get the number of classes to classify\n",
    "number_of_classes = target_test.shape[1]\n",
    "print(number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to Sam from the discussion for this. Theis fixes an error in the following step\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "import tensorflow as tf\n",
    "# Versions of tensorflow and keras seem incompatible with each other\n",
    "def _get_available_gpus():  \n",
    "\n",
    "    if tfback._LOCAL_DEVICES is None:  \n",
    "        devices = tf.config.list_logical_devices()  \n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]  \n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    " \n",
    "tfback._get_available_gpus = _get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin neural network\n",
    "network = Sequential()\n",
    "\n",
    "#Add convolutional layer with 64 filters, 5x5 window and ReLuU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5,5),\n",
    "                   input_shape=(channels,width,height),\n",
    "                   activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max pooling layer with a 2X2 window\n",
    "netowrk.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# add layer to flatten input\n",
    "network.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer of 128 units with a ReLUT activation function\n",
    "network.add(Dense(128,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another dropoutlayer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fully connected layer with a softmax activation funciton\n",
    "network.add(Dense(number_of_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',#root mean square propagation\n",
    "               metrics=['accuracy'])# accuracy of performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 52s 875us/step - loss: 0.5025 - accuracy: 0.8483 - val_loss: 0.1240 - val_accuracy: 0.9637\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 56s 935us/step - loss: 0.1508 - accuracy: 0.9555 - val_loss: 0.0749 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4fe779e80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "network.fit(features_train,\n",
    "           target_train,\n",
    "           epochs=2,\n",
    "           batch_size=1000,# # of observations)\n",
    "            validation_data=(features_test, target_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.9761999845504761% \n",
      " Error on training data: 0.023800015449523926\n"
     ]
    }
   ],
   "source": [
    "scores = network.evaluate(features_test, target_test, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
